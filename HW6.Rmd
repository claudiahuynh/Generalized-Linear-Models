---
title: "Homework 6"
output: github_document
---

```{r include = FALSE}
library(tidyverse)
library(p8105.datasets)
library(modelr)
library(GGally)
set.seed(1)
```

## Problem 1
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

Weâ€™ll focus on a simple linear regression with tmax as the response and tmin as the predictor, and are interested in the distribution of two quantities estimated from these data:


```{r}
set.seed(1)

bootstrap_results =
  weather_df |> 
  modelr::bootstrap(n = 5000) |> 
  mutate(
    models = map(strap, \(x) lm(tmax ~ tmin, data = x)),
    results = map(models, broom::tidy),
    r_squared = map(models, broom::glance)
  ) |> 
  unnest(results) |> 
  select(term, estimate, r_squared) |> 
  pivot_wider(
    names_from = term,
    values_from = estimate
  ) |> 
  rename(
    beta_1 = tmin,
    beta_0 = `(Intercept)`
  ) |> 
   mutate(
    log_beta_product = log(beta_0 * beta_1)
  ) |> 
  unnest(r_squared) |> 
  select(log_beta_product, adj.r.squared)
```

Plot the distribution of these two quantities. 
```{r}
r_squared_dist = 
  bootstrap_results |> 
  ggplot(aes(x = adj.r.squared)) +
  geom_density() +
  labs(
    title = "Distribution of adjusted R-squared",
    x = "Adjusted R_squared",
    y = "Density"
  )

print(r_squared_dist)


log_beta_product = 
  bootstrap_results |> 
  ggplot(aes(x = log_beta_product)) +
  geom_density() +
  labs(
    title = "Distribution of log beta products",
    x = "Log (Beta_0 * Beta_1)",
    y = "Density"
  )

print(log_beta_product)
```

The distributions of these quantities follow normal distribution. 

Compute a 95% confidence interval for these two quantities. 
```{r}
quantiles = 
  bootstrap_results |> 
  summarize(
    r2_0.025 = quantile(adj.r.squared, probs = 0.025),
    r2_0.975 = quantile(adj.r.squared, probs = 0.975),
    log_beta_0.025 = quantile(log_beta_product, probs = 0.025),
    log_beta_0.975 = quantile(log_beta_product, probs = 0.975),
  ) |> 
  mutate(
    r_squared_CI = str_c("(", round(r2_0.025, 3), ",", " ", round(r2_0.975, 3), ")"),
    log_beta_product_CI = str_c("(", round(log_beta_0.025, 3), ",", " ", round(log_beta_0.975, 3), ")")
  ) |> 
  select(r_squared_CI, log_beta_product_CI) 

quantiles |> 
  knitr::kable()

print(quantiles)
```

## Problem 2
```{r data wrangling}
homicide_df =
  read_csv("data/homicide-data.csv") |> 
  mutate(
    city_state = paste(city, state, sep = ", "),
    solved_bin = ifelse(disposition == "Closed by arrest", 1, 0),
    victim_age = as.numeric(victim_age)
  ) |>
  filter(
    !(city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL")),
    victim_race %in% c("White", "Black")
  ) |> 
  select(-city, -state)
```

Glm for Baltimore, MD
```{r}
baltimore = 
  homicide_df |> 
  filter(
    city_state == "Baltimore, MD"
  ) 

baltimore_glm = 
  glm(solved_bin ~ victim_age + victim_race + victim_sex, data = baltimore, family = binomial()) |> 
  broom::tidy() |> 
  filter(
    term == "victim_sexMale"
  ) |> 
  mutate(
    odds_ratio = exp(estimate),
    conf_upper_95 = exp(estimate + 1.96 * std.error),
    conf_lower_95 = exp(estimate - 1.96 * std.error)
  )
```

Do this for every city 

```{r}
glm_function = function(df) {
  df = 
    glm(solved_bin ~ victim_age + victim_race + victim_sex, data = df, family = binomial()) |> 
    broom::tidy() |> 
    filter(
      term == "victim_sexMale"
    ) |> 
    mutate(
      odds_ratio = exp(estimate),
      conf_upper_95 = exp(estimate + 1.96 * std.error),
      conf_lower_95 = exp(estimate - 1.96 * std.error)
    )
}

homicide_glm = 
  homicide_df |> 
  select(city_state, solved_bin, victim_sex, victim_age, victim_race) |>
  nest(data = solved_bin:victim_race) |> 
  mutate(
    glm_results = map(data, glm_function)
  ) |> 
  unnest(glm_results) |> 
  select(city_state, odds_ratio, conf_lower_95, conf_upper_95)

homicide_glm |> 
  knitr::kable()

homicide_glm |> 
  ggplot(aes(x = reorder(city_state, odds_ratio), y = odds_ratio)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf_lower_95, ymax = conf_upper_95)) +
  theme(
    axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1, size = 8)
  ) +
  labs(
    title = "Odds Ratio by City with Error Bars",
    x = "City, State",
    y = "Odds Ratio"
  )
```
The plot shows that the adjusted odds ratio for solving homicides comparing male victims to female victims keeping all other variables fixed are lowest in New York, NY, Baton Rouge, LA and Omaha, NE, and are highest in Fresno, CA, Stockton, CA and Albuquerque, NM. The error bars for Fresno, Stockton and Albuquerque are also the largest, indicating more uncertainty about the true adjusted odds ratio. The error bars in Chicago, IL, Philadelphia, OA and New Orleans, LA are the smallest, indicating more certainty about the true adjusted OR. Most cities have adjusted ORs concentrated around the range 0.5 to 1, indicating little to no difference in the likelihood in solving homicides between male and female victims.
## Problem 3
```{r}
bwt_df = 
  read_csv("data/birthweight.csv") |> 
  janitor::clean_names() |>
  mutate(
    babysex = as.factor(babysex),
    babysex = fct_recode(babysex, "male" = "1", "female" = "2"),
    frace = as.factor(frace),
    frace = fct_recode(
      frace, "white" = "1", "black" = "2", "asian" = "3", 
      "puerto rican" = "4", "other" = "8"),
    malform = as.logical(malform),
    mrace = as.factor(mrace),
    mrace = fct_recode(
      mrace, "white" = "1", "black" = "2", "asian" = "3", 
      "puerto rican" = "4"))

sum(is.na(bwt_df))
```
From a biological standpoint, I hypothesize that the following factors are more likely to underly birthweight: `babysex`, `bhead`, `blength`, `gaweeks`, `malform`, `smoken`. From a socioeconomical standpoint, I hypothesize that `frace` and `mrace` may also influence birthweight, because poverty, education, and employment tend to vary across different racial and ethnic groups.

Before fitting any models, I checked for NA values in the data frame. The `bwt` data frame contains 0 NA values. 
#### Create my own model
I will first fit MLR with these variables as predictors.
```{r}
linear_model = lm(bwt ~ babysex + bhead + blength + gaweeks + malform + smoken + mrace + frace, data = bwt_df)
summary(linear_model)
```

The results suggest that all covariates are significant predictors of birthweight except for `frace` and `malform`. The adjusted R-squared for this model is 0.7049. I will proceed with refitting the model without these covariates.

```{r}
refitted_linear_model = lm(bwt ~ babysex + bhead + blength + gaweeks + smoken + mrace, data = bwt_df) 

summary(refitted_linear_model)
```
The refitted model now shows an adjusted R-squared of 0.7051. This suggests that the refitted model does a slightly better job at explaining the variation in birthweight compared to the original model. 

I am also interested in looking at interactions between different covariates. I will use `ggpairs` to look at multicollinearity between the continuous variables in my model. 

```{r}
ggpairs(bwt_df[, c("bhead", "blength", "smoken", "gaweeks")])
```

Results from ggpairs suggest that `blength` has a moderate linear relationship with `bhead` (corr = 0.630), `gaweeks` has a moderately weak relationship with `blength` (corr = 0.359) and `bhead` (corr = 0.378). I will fit another model that includes the three-way interaction between these covariates. 


```{r}
interaction_model = lm(bwt ~ babysex + bhead * blength * gaweeks + smoken + mrace, data = bwt_df)
summary(interaction_model)

AIC(interaction_model)
AIC(refitted_linear_model)
```

Results from interaction model suggest that all covariates are significant predictors of birthweight, and that the interactions between `gaweek`, `bhead` and `blength` are also significant (p-values are all lower than 0.05). The adjusted R-squared for this model is 0.7072, which suggests that this model is slightly better than the refitted linear model without interaction terms. 

I also investigated AIC values to evaluate goodness-of-fit of these two models, and found that the AIC value for the interaction model is slightly lower than that for the refitted linear model. Based on adjusted R-squared and AIC value, the interaction model will be my final model. 

The plot showing predictions against residuals is as follows:
```{r}
bwt_plot = 
  bwt_df |> 
  add_predictions(interaction_model) |> 
  add_residuals(interaction_model) |> 
  ggplot(aes(x = pred, y = resid)) +
  geom_point() +
  labs(
    title = "Residuals vs. Predicted Birth Weight", x = "Predicted Birth Weight", y = "Residuals"
  )

print(bwt_plot)
```

The residual aginst prediction plot shows that there is an overall random scatter pattern, but the model does a better job at explaining variation in birth weight for birth weight values ranging from 2500 to 3500 grams, which is the typical birth weight for newborn babies. There are a few extreme data points around the range 500 to 1500, but this makes biological sense due to factors such as premature birth or maternal health conditions. 

#### Compare this model with other models. 

The first model, named `model1`, fits a MLR of `bwt` against `blength` and `gaweeks` as predictors. The second model, named `model2`, fits a MLR of `bwt` against `bhead`, `blength` and `babysex` including their three-way interaction. 
 
```{r}
model1 = lm(bwt ~ blength + gaweeks, data = bwt_df)
summary(model1)

model2 = lm(bwt ~ bhead * blength * babysex, data = bwt_df)
summary(model2)
```

Model 1 results show that both variables `blength` and `gaweeks` are significant predictors of birthweight, with both p-values being less than 2e-16. 

Model 2 results show that `bhead`, `blength` and `babysex` are all significant predictors of `bwt`. Based on interaction terms, those terms involving sex (`babysexfemale`) and the main predictors (`bhead`, `blength`) suggest that the effects of these variables on birthweight differ between male and female babies. Specifically, the three-way interaction (bhead:blength:babysexfemale) suggests that the effect of both head circumference and length at birth on birthweight is further modified by the baby's sex. This indicates that the relationship between `bwt` and main predictors `bhead` and `blength` are not the same for male and female babies. 

I will use `crossv_mc` to make comparisons between my model and the other models in terms of cross-validated prediction error. 

```{r}
cv_df = 
  crossv_mc(bwt_df, 100) |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )

cv_results_df = 
  cv_df |> 
  mutate(
    my_model = map(train, \(x) lm(bwt ~ babysex + bhead * blength * gaweeks + smoken + mrace, data = x)),
    model1 = map(train, \(x) lm(bwt ~ blength + gaweeks, data = x)), 
    model2 = map(train, \(x) lm(bwt ~ bhead * blength * babysex, data = x))
  ) |> 
  mutate(
    rmse_my_model = map2_dbl(my_model, test, rmse),
    rmse_model1 = map2_dbl(model1, test, rmse),
    rmse_model2 = map2_dbl(model2, test, rmse)
  ) |> 
  select(rmse_my_model, rmse_model1, rmse_model2)

```

Compare RMSE for three models.
```{r}
cv_results_df |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = ("rmse_")
  ) |> 
  ggplot(aes(x = model, y = rmse)) +
  geom_violin() +
  labs(
    title = "RMSE for 3 models", 
    x = "Model",
    y = "RMSE"
  )
```

The RMSE for the simple linear model (model 1) is the highest, indicating poor model fit. The RMSE for the model 2 is lower than that for model 1, and is slightly higher than that for my proposed model. The majority of RMSE data points for model 2 are concentrated around 287, and the majority of RMSE data points for my model are concentrated around 275. Overall, I think my model does well predicting birth weights among newborns. 
